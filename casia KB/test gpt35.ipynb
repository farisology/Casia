{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41696970-778b-48d0-b153-a9cafa8811d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farisology/.pyenv/versions/3.11.0/envs/casiaenv/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import openai\n",
    "import pinecone\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c565b35-cae6-4295-8e11-5c284b2ed848",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"--\"\n",
    "pinecone.init(api_key = \"--\", environment = 'us-east1-gcp')\n",
    "vdb = pinecone.Index(\"testbed\")\n",
    "\n",
    "def gpt3_embedding(content, engine='text-embedding-ada-002'):\n",
    "    content = content.encode(encoding='ASCII',errors='ignore').decode()  # fix any UNICODE errors\n",
    "    response = openai.Embedding.create(input=content,engine=engine)\n",
    "    vector = response['data'][0]['embedding']  # this is a normal list\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e0f709-1258-430f-bb51-21ad535a576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= \"what plants to grow in Malaysia?\"\n",
    "\n",
    "vector = gpt3_embedding(a)\n",
    "results = vdb.query(vector=vector, top_k=3, include_values=False, include_metadata=True, namespace=\"casia_kb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0ef5a6-6274-42fd-a92f-bc932de83775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filepath, content):\n",
    "    with open(filepath, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(content)\n",
    "\n",
    "def chadgpt_completion(question, context):\n",
    "    max_retry = 5\n",
    "    retry = 0\n",
    "    preample = \"\"\"You are casia a wise botanist with vast knowledge in gardening and great servitude to human well-being. You will read the context passage -inclosed with tripple quotes- and reflect on the question then provide a specific beautiful answer using simple words with care and interest in the human sustainable way of living. Its your responsibility to answer based on what you know, if the context is not clear don't make up any answer and just reply humble that you don't know and maybe you can answer the question in the future.\n",
    "\n",
    "Passage:\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model='gpt-3.5-turbo',\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": f\"{preample}\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{context}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "                ],\n",
    "                temperature=0,\n",
    "                top_p=1.0, max_tokens=500, frequency_penalty=0.0, presence_penalty=0.0,\n",
    "            )\n",
    "            text = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            text = re.sub('[\\r\\n]+', '\\n', text)\n",
    "            text = re.sub('[\\t ]+', ' ', text)\n",
    "            filename = '%s_gpt3.txt' % time()\n",
    "            if not os.path.exists('gpt3_logs'):\n",
    "                os.makedirs('gpt3_logs')\n",
    "            save_file('gpt3_logs/%s' % filename, preample +\n",
    "                      '\\n\\n==========\\n\\n' + text)\n",
    "            return text\n",
    "        except Exception as oops:\n",
    "            retry += 1\n",
    "            if retry >= max_retry:\n",
    "                return \"GPT3.5 turbo error: %s\" % oops\n",
    "            print('Error communicating with OpenAI:', oops)\n",
    "            sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027c852e-a1a7-4dd3-a727-0ce0b1fc7bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Malaysia has a tropical climate, which means that it is suitable for growing a wide variety of plants. Some plants that can be grown in Malaysia include:\\n1. Orchids - Malaysia is known for its beautiful orchids, which thrive in the warm and humid climate.\\n2. Banana trees - Bananas are a staple food in Malaysia, and banana trees are easy to grow in the tropical climate.\\n3. Herbs - Many herbs, such as lemongrass, ginger, and turmeric, grow well in Malaysia and are commonly used in Malaysian cuisine.\\n4. Tropical fruits - Malaysia is home to a wide variety of tropical fruits, including durian, rambutan, and mangosteen.\\n5. Ferns - Ferns are a popular ornamental plant in Malaysia and can be grown both indoors and outdoors.\\n6. Palm trees - Palm trees are a common sight in Malaysia and are used for both ornamental and commercial purposes.\\n7. Pitcher plants - Malaysia is home to a variety of carnivorous plants, including the famous pitcher plant, which is native to the country.\\n8. Bamboo - Bamboo is a versatile plant that can be used for construction, furniture, and even as a food source.\\nOverall, there are many plants that can be grown in Malaysia, and the country's warm and humid climate provides ideal growing conditions for a wide variety of plants.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chadgpt_completion(a, '\\n'.join([info[\"metadata\"][\"text\"] for info in results[\"matches\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ed3df-06cd-4680-af2d-bf973e8f84ba",
   "metadata": {},
   "source": [
    "## next question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbeb1faa-139e-4f72-b9ab-b42f798fbda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: This model's maximum context length is 4097 tokens. However, your messages resulted in 6099 tokens. Please reduce the length of the messages.\n",
      "Error communicating with OpenAI: This model's maximum context length is 4097 tokens. However, your messages resulted in 6099 tokens. Please reduce the length of the messages.\n",
      "Error communicating with OpenAI: This model's maximum context length is 4097 tokens. However, your messages resulted in 6099 tokens. Please reduce the length of the messages.\n",
      "Error communicating with OpenAI: This model's maximum context length is 4097 tokens. However, your messages resulted in 6099 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"GPT3.5 turbo error: This model's maximum context length is 4097 tokens. However, your messages resulted in 6099 tokens. Please reduce the length of the messages.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= \"How to take care of orchids?\"\n",
    "\n",
    "vector = gpt3_embedding(a)\n",
    "results = vdb.query(vector=vector, top_k=3, include_values=False, include_metadata=True, namespace=\"casia_kb\")\n",
    "chadgpt_completion(a, '\\n'.join([info[\"metadata\"][\"text\"] for info in results[\"matches\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6aea52-0a8d-43d9-9145-99fb3f3ccc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "llm = OpenAI(openai_api_key=\"PROVIDE OPENAI_API_KEY HERE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e20b8b-50a2-4445-830e-665c10720906",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key= \"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "389d5734-7669-4190-b352-4933419a1997",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m load_summarize_chain(llm\u001b[38;5;241m=\u001b[39mllm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatches\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/casiaenv/lib/python3.11/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` not supported when there is not exactly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone output key, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/casiaenv/lib/python3.11/site-packages/langchain/chains/base.py:107\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[1m> Entering new \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chain...\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     )\n\u001b[0;32m--> 107\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[1m> Finished \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chain.\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/casiaenv/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:49\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     48\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m---> 49\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: output}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/casiaenv/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:83\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, docs: List[Document], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stuff all documents into one prompt and pass to LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/casiaenv/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:62\u001b[0m, in \u001b[0;36mStuffDocumentsChain._get_inputs\u001b[0;34m(self, docs, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m doc_dicts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[0;32m---> 62\u001b[0m     base_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m}\n\u001b[1;32m     63\u001b[0m     base_info\u001b[38;5;241m.\u001b[39mupdate(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     64\u001b[0m     document_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     65\u001b[0m         k: base_info[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_prompt\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[1;32m     66\u001b[0m     }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "model = load_summarize_chain(llm=llm, chain_type=\"stuff\")\n",
    "model.run('\\n'.join([info[\"metadata\"][\"text\"] for info in results[\"matches\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8820c8-b9fc-4be0-a84e-e42130f5fa84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
